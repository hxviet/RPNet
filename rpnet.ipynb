{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hxviet/RPNet/blob/main/rpnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL3MzU0tTk5G"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, we're going to implement the deep learning model RPNet [1] with PyTorch to detect bounding boxes of and recognize characters on Chinese car license plates. We're going to use images of license plates from the Chinese City Parking Dataset updated in March 2019 (CCPD2019) [2]."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo instrucions\n",
        "\n",
        "RPNet consists of a detection module and a recognition module. I've trained a detection module for 5 epochs on nearly 200k images to achieve a 63% detection accuracy, and I used that pretrained detection module to train RPNet for 10 epochs, which achieve a 37% detection accuracy and 21% recognition accuracy. If you only want to test these two models, read all notes and run all cells in the following sections from top to bottom:\n",
        "1. **Preparing the environment**\n",
        "2. **Preparing data** &rarr; **Extracting dataset to disk**\n",
        "3. **Preparing data** &rarr; **Dataset constants**\n",
        "4. **Preparing data** &rarr; **Creating Dataset objects**\n",
        "5. **Defining the model**\n",
        "6. **Demo**\n",
        "\n",
        "(To see notebook sections, click the **Table of contents** button (![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB0AAAAVCAYAAAC6wOViAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAADfSURBVEhL7ZUxEkRAEEX/bqrE5CMndAJuIXQAArlDOIIDyBxD7gDkxKt6qkM9u6WGVbX7Enq6+PNbT3vEcfzCxTz5eil/0VMRG6muayilUJYlPM9DGIacMTPPM/q+52gfUbTrOjiOg6IoEEURsizjjJlhGPQzJkRRchkEwdtdH+Fe55RK6/s+R3YRnbZtqxsoz3Nd5jRNOWNmHEc0TcPRPqLTZVn4zj6iUyqv67qYpolX7CE6Xdf1FEFCFKUjkyQJR3a513CoqkqPPnoBdTEJfwJ9ksNj8Ey+8pf5FVFgA/aHTb4mZLeWAAAAAElFTkSuQmCC)) on Colab's left menu bar.)"
      ],
      "metadata": {
        "id": "1c8-IDa3Lyc6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4l74FLqiVDZ"
      },
      "source": [
        "## Acknowledgements\n",
        "\n",
        "A lot of code in this notebook were inspired by [2] (mostly for model definition) and [3] (mostly for training optimization) as well as the documentation of the libraries imported.\n",
        "\n",
        "I would like to express my gratitude to Dr. Do Ba Lam, my project advisor, and Mr. Tran The Anh at the School of Information and Communications Technology, Hanoi University of Science and Technology for their valuable advice and suggestions. I would also like to thank my father Hoang Xuan Hieu for sponsoring my Google Drive storage and Google Colab compute units.\n",
        "\n",
        "Hoang Xuan Viet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBBXBvVtn7Mr"
      },
      "source": [
        "# Preparing the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6No6UAGEU_Y"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBbpRDq8ufs8"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0NcauYEUPsy"
      },
      "outputs": [],
      "source": [
        "import os, math, torch\n",
        "from typing import Union, Callable, List, Tuple\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torchvision.io import read_image\n",
        "from torch import nn, optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as F\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchinfo import summary\n",
        "from torchvision.ops import roi_pool, box_iou, box_convert\n",
        "from torch.nn.functional import one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYNJolXTusAp"
      },
      "outputs": [],
      "source": [
        "def string_plate_num(t: Union[List[int], Tuple[int, ...], torch.Tensor]) -> str:\n",
        "    \"\"\"\n",
        "    Given a list, tuple, or tensor containing indices of the characters of a Chinese license plate,\n",
        "    returns the corresponding plate number string.\n",
        "    The represented Chinese license plate must be a Chinese character,\n",
        "    followed by an English letter, then followed by English letters or digits.\n",
        "    \"\"\"\n",
        "    char_0 = PROVINCES[t[0]]\n",
        "    char_1 = LETTERS[t[1]]\n",
        "    remaining_chars = [LETTERS_AND_DIGITS[e] for e in t[2:]]\n",
        "    return char_0 + char_1 + ''.join(remaining_chars)\n",
        "\n",
        "def show_images(images: List[torch.Tensor], labels: List[str]=None, max_ncols: int=4, figsize: Tuple[int, int]=None):\n",
        "    \"\"\"\n",
        "    Shows a grid of images\n",
        "    \"\"\"\n",
        "    ncols = min(len(images), max_ncols)\n",
        "    nrows = math.ceil(len(images) / ncols)\n",
        "    if not figsize:\n",
        "        figsize = (5 * ncols, 5 * nrows)\n",
        "    fig, axs = plt.subplots(nrows, ncols, squeeze=False, figsize=figsize)\n",
        "    for i, img in enumerate(images):\n",
        "        ax = axs[divmod(i, ncols)]\n",
        "        ax.imshow(F.to_pil_image(img.detach()))\n",
        "        ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "        if labels:\n",
        "            ax.set_title(labels[i], fontsize=24)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9bBUHtbaj0m"
      },
      "source": [
        "## Hardware"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIa2w54qeTqA"
      },
      "source": [
        "This notebook should be run with a CUDA-capable GPU. On Colab's top menu bar, go to **Runtime** &rarr; **Change runtime type**, and then under **Hardware accelerator**, choose **GPU**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lnLdJ7zltmg"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMHPYOSwEZcc"
      },
      "source": [
        "## The working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ3za0oIqOSQ"
      },
      "source": [
        "The working directory should\n",
        "* be in your Google Drive and\n",
        "* contain\n",
        "    * [the dataset archive](https://drive.google.com/drive/folders/1qLjlYAczIsjuCqIJIXo2d8VKbdix93BF?usp=share_link) or a shortcut thereto (Learn how to make a shortcut in your Google Drive [here](https://support.google.com/drive/answer/9700156?hl=en&co=GENIE.Platform%3DDesktop&oco=0).) and\n",
        "    * a directory named **models** storing trained models' parameters (You can make a copy of [this directory](https://drive.google.com/drive/folders/1TP8ecZtUBOB49D1CeCgMcjyjDBy3ugwJ?usp=share_link), which has the parameters of the models I trained. If you only want to test these models, you can make a shortcut to the directory instead of copying)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5MQp5dKePrF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u2H5mgo2238"
      },
      "source": [
        "To get the path to the directory you would like to be the working directory, click the **Files** button on Colab's left menu bar (![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB0AAAAVCAYAAAC6wOViAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAADSSURBVEhL7ZQxCoMwFIb/lo7Z9QDuWQWP4SjoGbyIm94gR/EA2XXXAzi3PHkgqW1MhdhC/SCEPwQ+8pK8SxzHdxzMledD+Yp0s7xhGCIIAk4LXddhmiZOn2GVklApxcmk73uUZblLbJUWRYE8zzmtGYYB4zhyMqFKNE3DycTpTulUWuvVeCcUQiBNUyRJwismN56t1HU9S1yRUqKqKkRRhLZteXXhf77MKfXK70rp6dM3cB2038buNuhClmVz13pmd8PfgrrVKyGxKfXB+Xo9AjwAhrVWITYnlzMAAAAASUVORK5CYII=)), expand **drive** &rarr; **MyDrive** &rarr; some more directories until you get to your desired working directory. Right-click on your desired working directory and click **Copy path**. Then, run the following cell and paste the path when asked for input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Am8tTB4xtOFR"
      },
      "outputs": [],
      "source": [
        "%cd {input('Enter path to desired working directory: ')}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "8EcIMcSqePrG"
      },
      "source": [
        "# Preparing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiDwP6J8n6S0"
      },
      "source": [
        "## Extracting dataset to disk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wGRgLiTnMSM"
      },
      "source": [
        "Because the dataset archive is quite large (about 12 GB), this will take around 10 to 25 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ep2qODVCn4Tt"
      },
      "outputs": [],
      "source": [
        "!tar -xf 'CCPD2019.tar.xz' -C '/content'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPCpjcTw6QUl"
      },
      "source": [
        "## Dataset constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0300iJdVBLx_"
      },
      "outputs": [],
      "source": [
        "TEST_SET_NAMES = ('blur', 'challenge', 'db', 'fn', 'rotate', 'tilt', 'weather')\n",
        "PROVINCES = (\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\")\n",
        "LETTERS = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W',\n",
        "             'X', 'Y', 'Z', 'O')\n",
        "LETTERS_AND_DIGITS = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n",
        "       'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zjw6RNFPy9S"
      },
      "source": [
        "## Creating `Dataset` objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdi43--gnsDA"
      },
      "outputs": [],
      "source": [
        "class CCPD2019(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            root: Union[str, os.PathLike],\n",
        "            img_transform: Callable=None,\n",
        "            bbox_transform: Callable=None,\n",
        "            plate_num_transform: Callable=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param root: path to the dataset (or a subset) directory\n",
        "        :param img_transform: transform to apply to images\n",
        "        :param bbox_transform: transform to apply to bounding box labels\n",
        "        :param plate_num_transform: transform to apply to license plate number labels\n",
        "        \"\"\"\n",
        "        self.img_transform = img_transform\n",
        "        self.bbox_transform = bbox_transform\n",
        "        self.plate_num_transform = plate_num_transform\n",
        "        self.img_paths = []\n",
        "        if not os.path.isdir(root):\n",
        "            raise NotADirectoryError(f'Not a directory: {root}')\n",
        "        print('Creating CCPD2019 object for data at', root, end='\\n')\n",
        "        for dirpath, subdirnames, filenames in os.walk(root):\n",
        "            self.img_paths.extend([os.path.join(dirpath, fn) for fn in filenames])\n",
        "            print('\\t Added', len(filenames), 'files from', dirpath)\n",
        "        print('\\t Done')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns three objects representing an image, the bounding box, and the plate number.\n",
        "\n",
        "        If no transformation is specified,\n",
        "        the image is a tensor of size (C, H, W) and dtype `torch.uint8`,\n",
        "        the bounding box is a tensor in (x_min, y_min, x_max, y_max) format, and\n",
        "        the plate number is a tensor containing the indices of the 7 characters in the license plate.\n",
        "        \"\"\"\n",
        "        img_path = self.img_paths[idx]\n",
        "        image = read_image(img_path).to(device)\n",
        "        annotations = os.path.basename(img_path)\n",
        "        area, tilt, bbox, vertices, plate_num, brightness, blurriness = annotations.split('-')\n",
        "        bbox = [[int(i) for i in point.split('&')] for point in bbox.split('_')]\n",
        "        bbox = torch.tensor(bbox, device=device).flatten()\n",
        "        plate_num = [int(i) for i in plate_num.split('_')]\n",
        "        plate_num = torch.tensor(plate_num, device=device)\n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "        if self.bbox_transform:\n",
        "            bbox = self.bbox_transform(bbox)\n",
        "        if self.plate_num_transform:\n",
        "            plate_num = self.plate_num_transform(plate_num)\n",
        "        return image, bbox, plate_num"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1FkM8FSUFXJ"
      },
      "source": [
        "We're going to apply some transformations to the data before feeding them to the neural network.\n",
        "* The images are going to be resized to 480x480 and converted to `torch.float32`.\n",
        "* The bounding box labels are going to be converted to $(c_x, c_y, w, h)$ format, where $c_x$ and $c_y$ are the coordinates of the bounding box center, and $w$ and $h$ are the width and height of the bounding box. All four numbers are between $0$ and $1$.\n",
        "* The plate number characters are going to be one-hot encoded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sigu0CnHFP72"
      },
      "outputs": [],
      "source": [
        "img_transform = transforms.Compose([\n",
        "    transforms.Resize((480, 480)),\n",
        "    transforms.Lambda(lambda img: F.convert_image_dtype(img, torch.float32))\n",
        "])\n",
        "\n",
        "bbox_transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda t: box_convert(t, in_fmt='xyxy', out_fmt='cxcywh')),\n",
        "    transforms.Lambda(lambda t: t / torch.tensor([720, 1160, 720, 1160], device=t.device))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikH-Gvdztx4V"
      },
      "outputs": [],
      "source": [
        "# for demo, this cell can be skipped\n",
        "train_set = CCPD2019(\n",
        "    root='/content/CCPD2019/ccpd_base',\n",
        "    img_transform=img_transform,\n",
        "    bbox_transform=bbox_transform\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHfvyELvcqH7"
      },
      "outputs": [],
      "source": [
        "test_sets = {}\n",
        "num_test_samples = 0\n",
        "for set_name in TEST_SET_NAMES:\n",
        "    test_set = CCPD2019(\n",
        "        root='/content/CCPD2019/ccpd_' + set_name,\n",
        "        img_transform=img_transform,\n",
        "        bbox_transform=bbox_transform\n",
        "    )\n",
        "    test_sets[set_name] = test_set\n",
        "    num_test_samples += len(test_set)\n",
        "print('Total number of test samples:', num_test_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyzUZVTuQBxp"
      },
      "source": [
        "## Creating `DataLoader` objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k091oVulRqwG"
      },
      "outputs": [],
      "source": [
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrIMZdckiCmY"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=False)\n",
        "print('Training set loader size:', len(train_loader), 'batches')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqik6GvFczcq"
      },
      "outputs": [],
      "source": [
        "test_loaders = {}\n",
        "for set_name in test_sets:\n",
        "    loader = DataLoader(test_sets[set_name], batch_size=batch_size, shuffle=True, pin_memory=False)\n",
        "    test_loaders[set_name] = loader\n",
        "    print(f'Test set \"{set_name}\" loader size: {len(loader)} batches')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0F40xNpjkiK"
      },
      "source": [
        "## Viewing some samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPhYpsBpb_q6"
      },
      "source": [
        "Let's look at the first batch of samples from the training set to make sure that we've loaded and transformed data correctly. We will draw the bounding boxes in red and print the license plate numbers on top of the images. The Chinese characters might not display because they're missing from the font used but it's okay because if the 6 other characters are correct, it's very almost certain that we've handled data properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VF1yYRwCxKqq"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_loader))\n",
        "images_with_box, plate_nums = [], []\n",
        "for i in range(batch_size):\n",
        "    image = F.convert_image_dtype(batch[0][i], torch.uint8)\n",
        "    bbox = box_convert(batch[1][i:i+1], 'cxcywh', 'xyxy') * 480\n",
        "    plate_num = string_plate_num(batch[2][i])\n",
        "    img_with_box = draw_bounding_boxes(image, bbox, colors=['red'], width=5)\n",
        "    images_with_box.append(img_with_box)\n",
        "    plate_nums.append(plate_num)\n",
        "show_images(images_with_box, plate_nums)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upyNIyyXjEoO"
      },
      "source": [
        "# Defining the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcTO52kK4KBa"
      },
      "source": [
        "The following figure [1] describes the architecture of RPNet.\n",
        "\n",
        "![The architecture of RPNet](https://drive.google.com/uc?export=view&id=1rXhiu4UqT7eFsyafS6Pp_skmWI47cJwQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iF5GbZEfKMU"
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.benchmark = True #let gpu benchmark convolution algorithms and choose the fastest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVTAvSRUK4KM"
      },
      "source": [
        "## The detection module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pslRxkhDRNSo"
      },
      "outputs": [],
      "source": [
        "class LPDetection(nn.Module):\n",
        "    \"\"\"\n",
        "    The detection module of RPNet\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LPDetection, self).__init__()\n",
        "        feature_map_0 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=48, kernel_size=5, stride=2, padding=2, bias=False),\n",
        "            nn.BatchNorm2d(num_features=48),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        feature_map_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=48, out_channels=64, kernel_size=5, stride=1, padding=2, bias=False),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        feature_map_2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=2, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        feature_map_3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=160, kernel_size=5, stride=1, padding=2, bias=False),\n",
        "            nn.BatchNorm2d(num_features=160),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        feature_map_4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=160, out_channels=192, kernel_size=5, stride=1, padding=2, bias=False),\n",
        "            nn.BatchNorm2d(num_features=192),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        feature_map_5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=192, out_channels=192, kernel_size=5, stride=1, padding=2, bias=False),\n",
        "            nn.BatchNorm2d(num_features=192),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        feature_map_6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=192, out_channels=192, kernel_size=5, stride=1, padding=2, bias=False),\n",
        "            nn.BatchNorm2d(num_features=192),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        feature_map_7 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=192, out_channels=192, kernel_size=5, stride=1, padding=2, bias=False),\n",
        "            nn.BatchNorm2d(num_features=192),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        feature_map_8 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=192, out_channels=192, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=192),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        feature_map_9 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=192, out_channels=192, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=192),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            feature_map_0,\n",
        "            feature_map_1,\n",
        "            feature_map_2,\n",
        "            feature_map_3,\n",
        "            feature_map_4,\n",
        "            feature_map_5,\n",
        "            feature_map_6,\n",
        "            feature_map_7,\n",
        "            feature_map_8,\n",
        "            feature_map_9\n",
        "        )\n",
        "        self.box_regressor = nn.Sequential(\n",
        "            nn.Linear(23232, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 4),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = nn.Flatten()(x)\n",
        "        x = self.box_regressor(x)\n",
        "        return x # tensor([cx, cy, w, h]) for each sample, with all 4 numbers in [0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOwDsNIbcufi"
      },
      "outputs": [],
      "source": [
        "detection_model = LPDetection().to(device)\n",
        "summary(detection_model, input_size=(batch_size, 3, 480, 480))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtjiZNXUK6qf"
      },
      "source": [
        "## The whole RPNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doHEDgvauJOO"
      },
      "outputs": [],
      "source": [
        "class RPNet(nn.Module):\n",
        "    \"\"\"\n",
        "    An end-to-end neural network for Chinese 7-character license plate detection and recognition\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(RPNet, self).__init__()\n",
        "        self.detection_module = LPDetection()\n",
        "        self.character_classifier_0 = nn.Sequential(\n",
        "            nn.Linear(53248, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, len(PROVINCES)),\n",
        "        )\n",
        "        self.character_classifier_1 = nn.Sequential(\n",
        "            nn.Linear(53248, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, len(LETTERS)),\n",
        "        )\n",
        "        self.character_classifier_2 = nn.Sequential(\n",
        "            nn.Linear(53248, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, len(LETTERS_AND_DIGITS)),\n",
        "        )\n",
        "        self.character_classifier_3 = nn.Sequential(\n",
        "            nn.Linear(53248, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, len(LETTERS_AND_DIGITS)),\n",
        "        )\n",
        "        self.character_classifier_4 = nn.Sequential(\n",
        "            nn.Linear(53248, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, len(LETTERS_AND_DIGITS)),\n",
        "        )\n",
        "        self.character_classifier_5 = nn.Sequential(\n",
        "            nn.Linear(53248, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, len(LETTERS_AND_DIGITS)),\n",
        "        )\n",
        "        self.character_classifier_6 = nn.Sequential(\n",
        "            nn.Linear(53248, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, len(LETTERS_AND_DIGITS)),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # get feature maps and bounding box\n",
        "        feature_extractor = self.detection_module.feature_extractor\n",
        "        x0 = feature_extractor[0](x)\n",
        "        x1 = feature_extractor[1](x0)\n",
        "        x2 = feature_extractor[2](x1)\n",
        "        x3 = feature_extractor[3](x2)\n",
        "        x4 = feature_extractor[4](x3)\n",
        "        x5 = feature_extractor[5](x4)\n",
        "        x6 = feature_extractor[6](x5)\n",
        "        x7 = feature_extractor[7](x6)\n",
        "        x8 = feature_extractor[8](x7)\n",
        "        x9 = feature_extractor[9](x8)\n",
        "        bbox = self.detection_module.box_regressor(nn.Flatten()(x9))\n",
        "        # extract RoIs from 2nd, 4th, and 6th feature maps\n",
        "        bbox_vertices = box_convert(bbox, 'cxcywh', 'xyxy').clamp(min=0, max=1)\n",
        "        bbox_vertices = list(bbox_vertices.unsqueeze(dim=1))\n",
        "        roi_1 = roi_pool(input=x1,\n",
        "                         boxes=bbox_vertices,\n",
        "                         output_size=(8, 16),\n",
        "                         spatial_scale=x1.size()[2])\n",
        "        roi_3 = roi_pool(input=x3,\n",
        "                         boxes=bbox_vertices,\n",
        "                         output_size=(8, 16),\n",
        "                         spatial_scale=x3.size()[2])\n",
        "        roi_5 = roi_pool(input=x5,\n",
        "                         boxes=bbox_vertices,\n",
        "                         output_size=(8, 16),\n",
        "                         spatial_scale=x5.size()[2])\n",
        "        rois = nn.Flatten()(torch.cat((roi_1, roi_3, roi_5), dim=1))\n",
        "        # classify characters\n",
        "        char_0 = self.character_classifier_0(rois)\n",
        "        char_1 = self.character_classifier_1(rois)\n",
        "        char_2 = self.character_classifier_2(rois)\n",
        "        char_3 = self.character_classifier_3(rois)\n",
        "        char_4 = self.character_classifier_4(rois)\n",
        "        char_5 = self.character_classifier_5(rois)\n",
        "        char_6 = self.character_classifier_6(rois)\n",
        "        return bbox, (char_0, char_1, char_2, char_3, char_4, char_5, char_6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKnd2jtzuBCJ"
      },
      "outputs": [],
      "source": [
        "rpnet = RPNet().to(device)\n",
        "summary(rpnet, input_size=(batch_size, 3, 480, 480))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iGMANq3me7e"
      },
      "source": [
        "# Training and testing the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saO9vQUsBGbe"
      },
      "source": [
        "Each model which we're going to train will be trained on the train set for several epochs. After each epoch, if the loss function averaged over the training set decreases, the model's new parameters are saved and tested.\n",
        "\n",
        "We're going to use the AdamW optimizer.\n",
        "\n",
        "Each training epoch will probably take somewhere between 30  and 90 minutes, depending on the GPU Colab allocates to you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbCpfb-wnWfj"
      },
      "source": [
        "## Pre-training the detection module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaGeZCUT-sXX"
      },
      "source": [
        "As explained in [1], we need to pre-train the detection module before training RPNet end-to-end so that the detection module can give reasonable bounding box predictions.\n",
        "\n",
        "The loss function for training the detection module is the Smooth L1 Loss between the predicted bounding box and the true bounding box in $(c_x, c_y, w, h)$ format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88ouxFyAeS-h"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=False)\n",
        "detection_criterion = nn.SmoothL1Loss()\n",
        "detection_optimizer = optim.AdamW(detection_model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZQfiLhd4KhW"
      },
      "outputs": [],
      "source": [
        "best_avg_train_loss = float('inf')\n",
        "\n",
        "for epoch in range(1, 11):\n",
        "    # train 1 epoch\n",
        "    print('--------------------')\n",
        "    print('Training epoch', epoch)\n",
        "    detection_model.train()\n",
        "    total_train_loss = 0\n",
        "    num_samples_trained = 0\n",
        "    for batch_idx, (images, true_boxes, true_plate_nums) in enumerate(train_loader):\n",
        "        detection_optimizer.zero_grad(set_to_none=True)\n",
        "        predicted_boxes = detection_model(images)\n",
        "        train_loss = detection_criterion(predicted_boxes, true_boxes)\n",
        "        total_train_loss += train_loss.detach()\n",
        "        train_loss.backward()\n",
        "        detection_optimizer.step()\n",
        "        num_samples_trained += len(images)\n",
        "        if batch_idx % 1000 == 0 or batch_idx == len(train_loader) - 1:\n",
        "            avg_train_loss = total_train_loss.item() / num_samples_trained\n",
        "            print('[{}/{} samples ({:.0f}%)]\\tAverage training detection loss: {:.6f}'.format(\n",
        "                num_samples_trained, len(train_loader.dataset),\n",
        "                100.0 * num_samples_trained / len(train_loader.dataset), avg_train_loss))\n",
        "    # save and test model if average training loss improves\n",
        "    if avg_train_loss < best_avg_train_loss:\n",
        "        best_avg_train_loss = avg_train_loss\n",
        "        torch.save(detection_model.state_dict(), 'models/detection_new_code.pth')\n",
        "        print('\\nTesting epoch', epoch)\n",
        "        detection_model.eval()\n",
        "        with torch.no_grad():\n",
        "            overall_test_accuracy = 0\n",
        "            # evaluate model on each test set\n",
        "            for test_set_name in TEST_SET_NAMES:\n",
        "                test_loader = test_loaders[test_set_name]\n",
        "                test_accuracy = 0\n",
        "                for images, true_boxes, true_plate_nums in test_loader:\n",
        "                    predicted_boxes = detection_model(images)\n",
        "                    true_bbox_vertices = box_convert(true_boxes, 'cxcywh', 'xyxy')\n",
        "                    predicted_bbox_vertices = box_convert(predicted_boxes, 'cxcywh', 'xyxy')\n",
        "                    IoUs = box_iou(predicted_bbox_vertices, true_bbox_vertices).diagonal()\n",
        "                    test_accuracy += (IoUs > 0.7).sum().item()\n",
        "                overall_test_accuracy += test_accuracy\n",
        "                test_accuracy /= len(test_loader.dataset)\n",
        "                print(f'Set \"{test_set_name}\" detection accuracy: {100.0 * test_accuracy:.2f}%')\n",
        "            overall_test_accuracy /= num_test_samples\n",
        "            print(f'Overall test detection accuracy: {100.0 * overall_test_accuracy:.2f}%')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1Rm58pbeEFs"
      },
      "source": [
        "## Training RPNet end-to-end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M65gmhrq_lv0"
      },
      "source": [
        "We're going to load pretrained weights to the detection module of RPNet and train RPNet end-to-end. The loss function will be the sum of detection loss and recognition loss, where detection loss is the same loss function we used in training the detection module and recognition loss is the Cross Entropy Loss between predicted and true character distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GRD7CMB-ING"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=False)\n",
        "rpnet.detection_module.load_state_dict(torch.load('models/detection.pth'))\n",
        "detection_criterion = nn.SmoothL1Loss()\n",
        "recognition_criterion = nn.CrossEntropyLoss()\n",
        "rpnet_optimizer = optim.AdamW(rpnet.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmylQyGkKpgQ"
      },
      "outputs": [],
      "source": [
        "best_avg_train_loss = float('inf')\n",
        "\n",
        "for epoch in range(1, 11):\n",
        "    # train 1 epoch\n",
        "    print('--------------------')\n",
        "    print('Training epoch', epoch)\n",
        "    rpnet.train()\n",
        "    total_train_loss = 0\n",
        "    num_samples_trained = 0\n",
        "    for batch_idx, (images, true_boxes, true_plate_nums) in enumerate(train_loader):\n",
        "        rpnet_optimizer.zero_grad(set_to_none=True)\n",
        "        predicted_boxes, predicted_plate_nums = rpnet(images)\n",
        "        train_loss = detection_criterion(predicted_boxes, true_boxes)\n",
        "        for i in range(7):\n",
        "            train_loss += recognition_criterion(predicted_plate_nums[i], true_plate_nums[:, i])\n",
        "        total_train_loss += train_loss.detach()\n",
        "        train_loss.backward()\n",
        "        rpnet_optimizer.step()\n",
        "        num_samples_trained += len(images)\n",
        "        if batch_idx % 1000 == 0 or batch_idx == len(train_loader) - 1:\n",
        "            avg_train_loss = total_train_loss.item() / num_samples_trained\n",
        "            print('[{}/{} samples ({:.0f}%)]\\tAverage training recognition loss: {:.6f}'.format(\n",
        "                num_samples_trained, len(train_loader.dataset),\n",
        "                100.0 * num_samples_trained / len(train_loader.dataset), avg_train_loss))\n",
        "    # save and test model if average training loss improves\n",
        "    if avg_train_loss < best_avg_train_loss:\n",
        "        best_avg_train_loss = avg_train_loss\n",
        "        torch.save(rpnet.state_dict(), f'models/rpnet_epoch{epoch}.pth')\n",
        "        print('\\nTesting epoch', epoch)\n",
        "        rpnet.eval()\n",
        "        with torch.no_grad():\n",
        "            overall_detection_accuracy, overall_recognition_accuracy = 0, 0\n",
        "            # evaluate model on each test set\n",
        "            for test_set_name in TEST_SET_NAMES:\n",
        "                test_loader = test_loaders[test_set_name]\n",
        "                detection_accuracy, recognition_accuracy = 0, 0\n",
        "                for images, true_boxes, true_plate_nums in test_loader:\n",
        "                    predicted_boxes, predicted_plate_nums = rpnet(images)\n",
        "                    true_bbox_vertices = box_convert(true_boxes, 'cxcywh', 'xyxy')\n",
        "                    predicted_bbox_vertices = box_convert(predicted_boxes, 'cxcywh', 'xyxy')\n",
        "                    predicted_plate_nums = [t.argmax(dim=1, keepdim=True) for t in predicted_plate_nums]\n",
        "                    predicted_plate_nums = torch.hstack(predicted_plate_nums)\n",
        "                    IoUs = box_iou(predicted_bbox_vertices, true_bbox_vertices).diagonal()\n",
        "                    detection_accuracy += (IoUs > 0.7).sum().item()\n",
        "                    recog_acc_cond1 = IoUs > 0.6\n",
        "                    recog_acc_cond2 = predicted_plate_nums.eq(true_plate_nums).sum(dim=1) == 7\n",
        "                    recognition_accuracy += (recog_acc_cond1 * recog_acc_cond2).sum().item()\n",
        "                overall_detection_accuracy += detection_accuracy\n",
        "                overall_recognition_accuracy += recognition_accuracy\n",
        "                detection_accuracy /= len(test_loader.dataset)\n",
        "                recognition_accuracy /= len(test_loader.dataset)\n",
        "                print(f'Set \"{test_set_name}\" \\t detection accuracy: {100.0 * detection_accuracy:.2f}%', end='\\t')\n",
        "                print(f'recognition accuracy: {100.0 * recognition_accuracy:.2f}%')\n",
        "            overall_detection_accuracy /= num_test_samples\n",
        "            overall_recognition_accuracy /= num_test_samples\n",
        "            print(f'Overall test detection accuracy: {100.0 * overall_detection_accuracy:.2f}%')\n",
        "            print(f'Overall test recognition accuracy: {100.0 * overall_recognition_accuracy:.2f}%')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu-IKZG4eBWn"
      },
      "source": [
        "# Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AlDqSDz9gMZ"
      },
      "source": [
        "This section lets you test a trained detection module and RPNet on an image with a Chinese car license plate.\n",
        "\n",
        "There are some cells above you need to run before running those below; check out the **Introduction** section for details if you haven't."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load the models"
      ],
      "metadata": {
        "id": "JXqkv-3lJJ8y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ2xnF66anzL"
      },
      "source": [
        "If you've run the cells that need to be executed before those in this section as specified at the beginning of the notebook, first, run the following cell to load a set of trained parameters to RPNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYl-3rQoBlvr"
      },
      "outputs": [],
      "source": [
        "detection_model.load_state_dict(torch.load('models/detection.pth'))\n",
        "rpnet.load_state_dict(torch.load('models/rpnet_epoch10.pth'))\n",
        "detection_model.eval()\n",
        "rpnet.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Select an image"
      ],
      "metadata": {
        "id": "vZChUl7zJRgd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8vAMrM-Zr8r"
      },
      "source": [
        "Next, either run the following cell to choose a random image from one of the test sets..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFD7eoK3ZXrQ"
      },
      "outputs": [],
      "source": [
        "rand_test_set_name = TEST_SET_NAMES[torch.randint(len(TEST_SET_NAMES), size=(1,)).item()]\n",
        "rand_test_set = test_sets[rand_test_set_name]\n",
        "print(f'Random image chosen from test set \"{rand_test_set_name}\"')\n",
        "image, true_bbox, true_plate_num = rand_test_set[torch.randint(len(rand_test_set), size=(1,)).item()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfpad2z_bGPZ"
      },
      "source": [
        "... or run the following cell to use an image you've uploaded to Colab disk. Make sure that your image contains exactly one 7-character Chinese car license plate. To upload an image to Colab disk, click the **Files** button on Colab's left menu bar (![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABYAAAASCAYAAABfJS4tAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAADGSURBVDhP7ZSxDoMgEECvTSdWmVmZ3fwBVvkb/Qa+RP7G2RVXmR3bgCetpaaYyNCkLzHkTvJy6B2XqqrukIErrqezqZhSCkVRYPTEGAPzPGOURhA7qVLKJ98Zx9G/OyIPYikl1HXtk5+YpgmstRhtcSfSWmO0EH1jV90wDNGzJyWEgBACyrLEzMIN10DXdV6UCuccmqYBxhj0fY/ZjF3xFwd+Txy1m2ubI+ztTxrpFNq29dO5knQJfcNN5avUsRGfSaafB/AAU0xQLAdsmA8AAAAASUVORK5CYII=)), then click the **Upload to session storage** button (![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABsAAAAeCAYAAADdGWXmAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAECSURBVEhL7ZaxDYQwDEV919GzA4yARM0O1LADQ9BCC9swBEMww50cOQKd4mBDoDjxmiSG6PPJx+KVZdkHbuJN4y08YkFgA5IkCVRVBXEcU8XNNE0wjiOt/LDO6rreFULyPDcPJYF1NgyDGVGUw96DSBwGOzOJw6AB2RMMnkaf4CXRR0EXpwLiwrfvEmcch8Q039YWtZgVwrEoCqrKUIn9OirLkg2DC7EY9+qsSwlisTRNzbhtSXZur+0hFsNIt21reqAF51jb9kgfqjOb55lmK64ah0rsLIfEsDtoOwvCtqu+7yGKIlrpWJYFmqah1QrrrOs6s0kL7uEC8/ykBuFfxQC+iytbsa6eT3IAAAAASUVORK5CYII=))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzFecF3BbFz0"
      },
      "outputs": [],
      "source": [
        "img_path = input('Enter path to image: ')\n",
        "image = img_transform(read_image(img_path).to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Get predictions"
      ],
      "metadata": {
        "id": "qL2Q3lYJJXL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, run the following cell to get the detection module's predicted bounding box."
      ],
      "metadata": {
        "id": "F7JxVv5nIRkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    pred_bbox= detection_model(image.unsqueeze(0))\n",
        "    print('Bounding box in (cx, cy, w, h) format predicted by the detection module: ', end='')\n",
        "    print([round(i.item(), 4) for i in pred_bbox.squeeze()])\n",
        "    pred_bbox_vertices = box_convert(pred_bbox, 'cxcywh', 'xyxy') * 480\n",
        "    converted_img = F.convert_image_dtype(image, torch.uint8)\n",
        "    img_with_box = draw_bounding_boxes(converted_img, pred_bbox_vertices, colors=['red'], width=5)\n",
        "    show_images([img_with_box], max_ncols=1, figsize=(10, 10))"
      ],
      "metadata": {
        "id": "n-PnPAgOIQ7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_stBgmik8lX"
      },
      "source": [
        "Finally, run this cell below to get RPNet's predicted bounding box and plate number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLlXb0Lk_9iO"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    pred_bbox, pred_plate_num = rpnet(image.unsqueeze(0))\n",
        "    print('Bounding box in (cx, cy, w, h) format predicted by RPNet: ', end='')\n",
        "    print([round(i.item(), 4) for i in pred_bbox.squeeze()])\n",
        "    pred_bbox_vertices = box_convert(pred_bbox, 'cxcywh', 'xyxy') * 480\n",
        "    print('Plate number predicted by RPNet:', string_plate_num([t.argmax() for t in pred_plate_num]))\n",
        "    converted_img = F.convert_image_dtype(image, torch.uint8)\n",
        "    img_with_box = draw_bounding_boxes(converted_img, pred_bbox_vertices, colors=['red'], width=5)\n",
        "    show_images([img_with_box], max_ncols=1, figsize=(10, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eymMQycj5nV3"
      },
      "source": [
        "# References\n",
        "\n",
        "1. Xu, Z. et al. (2018). Towards End-to-End License Plate Detection and Recognition: A Large Dataset and Baseline. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds) Computer Vision – ECCV 2018. ECCV 2018. Lecture Notes in Computer Science(), vol 11217. Springer, Cham. https://doi.org/10.1007/978-3-030-01261-8_16\n",
        "\n",
        "2. https://github.com/detectRecog/CCPD\n",
        "\n",
        "3. Lin, J. (2022). Optimize PyTorch Performance for Speed and Memory Efficiency. Towards Data Science. https://towardsdatascience.com/optimize-pytorch-performance-for-speed-and-memory-efficiency-2022-84f453916ea6"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}